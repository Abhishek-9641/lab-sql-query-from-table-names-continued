{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a57394-53dd-47cd-a5e7-6e7122bbb0f6",
   "metadata": {},
   "source": [
    "# SQL query from table names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc2813-763c-49f2-9f5d-e1f87a20556f",
   "metadata": {
    "id": "86bc2813-763c-49f2-9f5d-e1f87a20556f"
   },
   "source": [
    "In This notebook we are going to test if using just the name of the table, and a shord definition of its contect we can use a model like GTP3.5-Turbo to select which tables are necessary to create a SQL Order to answer the user petition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbbdf5b-4d7f-4496-8f40-9d15ea46d023",
   "metadata": {
    "id": "7fbbdf5b-4d7f-4496-8f40-9d15ea46d023"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e480bbfb-9a80-4ea6-b792-067e63ae3148",
   "metadata": {
    "id": "e480bbfb-9a80-4ea6-b792-067e63ae3148"
   },
   "outputs": [],
   "source": [
    "#Functio to call the model.\n",
    "def return_OAI(user_message):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "    context = []\n",
    "    context.append({'role':'system', \"content\": user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=context,\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5d2bdc-d822-4ed8-815e-b3f223730f15",
   "metadata": {
    "id": "6d5d2bdc-d822-4ed8-815e-b3f223730f15",
    "outputId": "61068bf0-41e3-40d9-b453-b76da5b0f086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      table                                         definition\n",
      "0     users    Stores user information such as id, name, email\n",
      "1    orders  Stores order details such as order_id, user_id...\n",
      "2  products  Stores product information such as product_id,...\n"
     ]
    }
   ],
   "source": [
    "#Definition of the tables.\n",
    "import pandas as pd\n",
    "\n",
    "# Table and definitions sample\n",
    "data = {'table':[\"users\", \"orders\", \"products\"],\n",
    "        'definition': [\"Stores user information such as id, name, email\",\n",
    "        \"Stores order details such as order_id, user_id, total_price\",\n",
    "        \"Stores product information such as product_id, name, price\"]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009e681c-d95d-4c9c-b044-5bfd76e3ad95",
   "metadata": {
    "id": "009e681c-d95d-4c9c-b044-5bfd76e3ad95"
   },
   "outputs": [],
   "source": [
    "text_tables = '\\n'.join([f\"{row['table']}: {row['definition']}\" for index, row in df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe03ac25-8d02-4cd1-99a1-be4220c6fd2d",
   "metadata": {
    "id": "fe03ac25-8d02-4cd1-99a1-be4220c6fd2d",
    "outputId": "c1f3aab1-5f26-48fe-fcf9-3780120f5aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: Stores user information such as id, name, email\n",
      "orders: Stores order details such as order_id, user_id, total_price\n",
      "products: Stores product information such as product_id, name, price\n"
     ]
    }
   ],
   "source": [
    "print(text_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e275ae-f20d-4134-b9b6-d8677dfb544c",
   "metadata": {
    "id": "c7e275ae-f20d-4134-b9b6-d8677dfb544c"
   },
   "outputs": [],
   "source": [
    "prompt_question_tables = \"\"\"\n",
    "Given the following tables and their content definitions,\n",
    "###Tables\n",
    "{tables}\n",
    "\n",
    "Tell me which tables would be necessary to query with SQL to address the user's question below.\n",
    "Return the table names in a json format.\n",
    "###User Questyion:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "user_query = \"Find the total price of all orders for a given user.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1cb5957-2df2-4e5e-9e6a-ace955c9817e",
   "metadata": {
    "id": "b1cb5957-2df2-4e5e-9e6a-ace955c9817e"
   },
   "outputs": [],
   "source": [
    "#Creating the prompt, with the user questions and the tables definitions.\n",
    "pqt1 = prompt_question_tables.format(tables=text_tables, question=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d30f2b-6c23-4fd6-8038-840fba784cce",
   "metadata": {
    "id": "10d30f2b-6c23-4fd6-8038-840fba784cce",
    "outputId": "9924022c-7b2b-4ec8-e2c2-75bc1c745151",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tables\": [\"users\", \"orders\", \"products\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(return_OAI(pqt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57e07083-be8f-4cd0-95bd-c4b909422c6b",
   "metadata": {
    "id": "57e07083-be8f-4cd0-95bd-c4b909422c6b"
   },
   "outputs": [],
   "source": [
    "user_query = \"Find the average price of products sold in the last month.\"\n",
    "pqt3 = prompt_question_tables.format(tables=text_tables,\n",
    "                                     question=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeb79e-caf1-4f48-9897-168d95d2ae37",
   "metadata": {
    "id": "a0eeb79e-caf1-4f48-9897-168d95d2ae37",
    "outputId": "81d77115-9cad-4284-a228-5368bb9aa6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tables\": [\"orders\", \"products\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(return_OAI(pqt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fd7d7",
   "metadata": {},
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try a few versions if you have time\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f61f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"tables\": [\"users\", \"orders\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# User wants to know all users who placed orders\n",
    "user_query = \"List all users who have placed at least one order.\"\n",
    "pqt2 = prompt_question_tables.format(\n",
    "    tables=text_tables,\n",
    "    question=user_query\n",
    ")\n",
    "print(return_OAI(pqt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45e8f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"tables\": [\"products\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# User wants product info\n",
    "user_query = \"Show me the names and prices of all products.\"\n",
    "pqt4 = prompt_question_tables.format(\n",
    "    tables=text_tables,\n",
    "    question=user_query\n",
    ")\n",
    "print(return_OAI(pqt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6640c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tables\": [\"users\", \"orders\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# User wants to find inactive users\n",
    "user_query = \"Find users who have not placed any orders.\"\n",
    "pqt6 = prompt_question_tables.format(\n",
    "    tables=text_tables,\n",
    "    question=user_query\n",
    ")\n",
    "print(return_OAI(pqt6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e257d26",
   "metadata": {},
   "source": [
    "Report on SQL Prompting with GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8ef17",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this lab, I tested how GPT can turn a normal question into SQL queries by using the names of tables. The idea was to see if different prompts would change the results, and to check when GPT gave good answers or when it made mistakes.\n",
    "\n",
    "What Worked Well\n",
    "\n",
    "When I gave GPT clear instructions with the right table names, it usually worked fine. For example, when I asked about the average price of products sold last month, it picked the right tables like users, orders, and products. The answers made sense, and the format was easy to read.\n",
    "\n",
    "What Didn’t Work Well\n",
    "\n",
    "Some prompts did not go as planned. If the question was vague or I didn’t explain the tables properly, GPT sometimes made up things. For example, it returned tables like “employees,” “salary,” or “studies” that were never part of the schema. That showed me GPT can “hallucinate” and invent stuff when it is unsure.\n",
    "\n",
    "Another small problem was that the output format was not always the same. Sometimes GPT gave plain JSON, and other times it wrapped it inside code blocks. This is not a big deal, but it could be annoying if I wanted to parse the results automatically.\n",
    "\n",
    "What I Learned\n",
    "\n",
    "I learned that the way I write the prompt makes a big difference. If I am very clear and give full information, GPT does much better. If I leave gaps, GPT tries to fill them but is often wrong. I also learned that I can’t just trust the first answer—it’s important to check and validate the output.\n",
    "\n",
    "Overall, this lab showed me that GPT is a useful tool for generating SQL, but it needs careful prompts and human oversight. The main lesson is that good prompts = good results, while unclear prompts = hallucinations or mistakes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
